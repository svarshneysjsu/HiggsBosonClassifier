{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split for Higgs Boson Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SparkContext and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/12 00:57:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/12 00:57:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/12 00:57:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGING_DATA_FOLDER = \"staging_data\"\n",
    "CLEAN_DATA_FOLDER = \"clean_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read CSV, transform and load to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(os.path.join(STAGING_DATA_FOLDER, \"clean.csv\"))\n",
    "rdd = rdd.map(\n",
    "    lambda row: [\n",
    "        int(float(v)) if i == 0 else float(v) for i, v in enumerate(row.split(\",\"))\n",
    "    ]\n",
    ")\n",
    "df = rdd.toDF(\n",
    "    schema=[\n",
    "        \"signal\",\n",
    "        \"lepton pT\",\n",
    "        \"lepton eta\",\n",
    "        \"lepton phi\",\n",
    "        \"missing energy magnitude\",\n",
    "        \"missing energy phi\",\n",
    "        \"jet 1 pt\",\n",
    "        \"jet 1 eta\",\n",
    "        \"jet 1 phi\",\n",
    "        \"jet 1 b-tag\",\n",
    "        \"jet 2 pt\",\n",
    "        \"jet 2 eta\",\n",
    "        \"jet 2 phi\",\n",
    "        \"jet 2 b-tag\",\n",
    "        \"jet 3 pt\",\n",
    "        \"jet 3 eta\",\n",
    "        \"jet 3 phi\",\n",
    "        \"jet 3 b-tag\",\n",
    "        \"jet 4 pt\",\n",
    "        \"jet 4 eta\",\n",
    "        \"jet 4 phi\",\n",
    "        \"jet 4 b-tag\",\n",
    "        \"m_jj\",\n",
    "        \"m_jjj\",\n",
    "        \"m_lv\",\n",
    "        \"m_jlv\",\n",
    "        \"m_bb\",\n",
    "        \"m_wbb\",\n",
    "        \"m_wwbb\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Dataframe into 70:30 for Train and Test respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 00:57:50 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/05/12 00:57:59 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df.coalesce(1).write.format(\"csv\").option(\"header\", \"false\").mode(\n",
    "    \"overwrite\"\n",
    ").save(os.path.join(CLEAN_DATA_FOLDER, \"train\"))\n",
    "files = os.listdir(os.path.join(CLEAN_DATA_FOLDER, \"train\"))\n",
    "csv_file = next((file for file in files if file.endswith(\".csv\")), None)\n",
    "os.rename(\n",
    "    os.path.join(CLEAN_DATA_FOLDER, os.path.join(\"train\", csv_file)),\n",
    "    os.path.join(CLEAN_DATA_FOLDER, \"train.csv\"),\n",
    ")\n",
    "shutil.rmtree(os.path.join(CLEAN_DATA_FOLDER, \"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_df.coalesce(1).write.format(\"csv\").option(\"header\", \"false\").mode(\n",
    "    \"overwrite\"\n",
    ").save(os.path.join(CLEAN_DATA_FOLDER, \"test\"))\n",
    "files = os.listdir(os.path.join(CLEAN_DATA_FOLDER, \"test\"))\n",
    "csv_file = next((file for file in files if file.endswith(\".csv\")), None)\n",
    "os.rename(\n",
    "    os.path.join(CLEAN_DATA_FOLDER, os.path.join(\"test\", csv_file)),\n",
    "    os.path.join(CLEAN_DATA_FOLDER, \"test.csv\"),\n",
    ")\n",
    "shutil.rmtree(os.path.join(CLEAN_DATA_FOLDER, \"test\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
